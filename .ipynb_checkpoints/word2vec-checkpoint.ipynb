{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask',model='bert-base-uncased', top_k=50)\n",
    "options_complete = unmasker(\"I visited a [MASK] country.\")\n",
    "options = []\n",
    "for doc in options_complete:\n",
    "    options.append(doc['token_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-05 11:32:21,703 : INFO : reading database/New_York_Times.txt...\n",
      "2021-02-05 11:32:30,117 : INFO : read 10000 paragraphs\n",
      "2021-02-05 11:32:37,847 : INFO : read 20000 paragraphs\n",
      "2021-02-05 11:32:46,462 : INFO : read 30000 paragraphs\n",
      "2021-02-05 11:32:55,248 : INFO : read 40000 paragraphs\n",
      "2021-02-05 11:33:04,196 : INFO : read 50000 paragraphs\n",
      "2021-02-05 11:33:12,680 : INFO : read 60000 paragraphs\n",
      "2021-02-05 11:33:21,621 : INFO : read 70000 paragraphs\n",
      "2021-02-05 11:33:29,500 : INFO : read 80000 paragraphs\n",
      "2021-02-05 11:33:37,535 : INFO : read 90000 paragraphs\n",
      "2021-02-05 11:33:45,911 : INFO : read 100000 paragraphs\n",
      "2021-02-05 11:33:55,377 : INFO : read 110000 paragraphs\n",
      "2021-02-05 11:34:04,307 : INFO : read 120000 paragraphs\n",
      "2021-02-05 11:34:14,426 : INFO : read 130000 paragraphs\n",
      "2021-02-05 11:34:21,797 : INFO : read 140000 paragraphs\n",
      "2021-02-05 11:34:29,733 : INFO : read 150000 paragraphs\n",
      "2021-02-05 11:34:37,597 : INFO : read 160000 paragraphs\n",
      "2021-02-05 11:34:46,236 : INFO : read 170000 paragraphs\n",
      "2021-02-05 11:34:54,565 : INFO : read 180000 paragraphs\n",
      "2021-02-05 11:35:02,180 : INFO : read 190000 paragraphs\n",
      "2021-02-05 11:35:10,688 : INFO : read 200000 paragraphs\n",
      "2021-02-05 11:35:19,413 : INFO : read 210000 paragraphs\n",
      "2021-02-05 11:35:29,413 : INFO : read 220000 paragraphs\n",
      "2021-02-05 11:35:39,085 : INFO : read 230000 paragraphs\n",
      "2021-02-05 11:36:15,008 : INFO : read 240000 paragraphs\n",
      "2021-02-05 11:36:34,465 : INFO : read 250000 paragraphs\n",
      "2021-02-05 11:36:42,483 : INFO : read 260000 paragraphs\n",
      "2021-02-05 11:36:51,034 : INFO : read 270000 paragraphs\n",
      "2021-02-05 11:37:00,009 : INFO : read 280000 paragraphs\n",
      "2021-02-05 11:37:09,125 : INFO : read 290000 paragraphs\n",
      "2021-02-05 11:37:18,143 : INFO : read 300000 paragraphs\n",
      "2021-02-05 11:37:27,454 : INFO : read 310000 paragraphs\n",
      "2021-02-05 11:37:37,414 : INFO : read 320000 paragraphs\n",
      "2021-02-05 11:37:46,923 : INFO : read 330000 paragraphs\n",
      "2021-02-05 11:37:56,731 : INFO : read 340000 paragraphs\n",
      "2021-02-05 11:38:07,011 : INFO : read 350000 paragraphs\n",
      "2021-02-05 11:38:17,170 : INFO : read 360000 paragraphs\n",
      "2021-02-05 11:38:26,913 : INFO : read 370000 paragraphs\n",
      "2021-02-05 11:38:36,481 : INFO : read 380000 paragraphs\n",
      "2021-02-05 11:38:46,327 : INFO : read 390000 paragraphs\n",
      "2021-02-05 11:38:56,077 : INFO : read 400000 paragraphs\n",
      "2021-02-05 11:46:26,615 : INFO : read 410000 paragraphs\n",
      "2021-02-05 11:46:39,319 : INFO : read 420000 paragraphs\n",
      "2021-02-05 11:46:49,963 : INFO : read 430000 paragraphs\n",
      "2021-02-05 11:47:00,493 : INFO : read 440000 paragraphs\n",
      "2021-02-05 11:47:11,921 : INFO : read 450000 paragraphs\n",
      "2021-02-05 11:47:22,115 : INFO : read 460000 paragraphs\n",
      "2021-02-05 11:47:32,659 : INFO : read 470000 paragraphs\n",
      "2021-02-05 11:47:43,574 : INFO : read 480000 paragraphs\n",
      "2021-02-05 11:47:54,345 : INFO : read 490000 paragraphs\n",
      "2021-02-05 11:48:05,023 : INFO : read 500000 paragraphs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-a2c6d7d7bc55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimple_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mdocuments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mread_input\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Done reading data file\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-a2c6d7d7bc55>\u001b[0m in \u001b[0;36mread_input\u001b[1;34m(input_file)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mparagraph\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[0mi\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "data_file='database/New_York_Times.txt'\n",
    "\n",
    "def read_input(input_file):\n",
    "    \n",
    "    logging.info(f\"reading {input_file}...\")\n",
    "    \n",
    "    with open(input_file, 'r') as file:\n",
    "        i=0\n",
    "        for paragraph in file:\n",
    "            i+=1\n",
    "            if i%10000==0:\n",
    "                logging.info (f\"read {i} paragraphs\")\n",
    "            if i>100000:\n",
    "                break\n",
    "            yield gensim.utils.simple_preprocess(paragraph)\n",
    "\n",
    "documents = list (read_input (data_file))\n",
    "logging.info (\"Done reading data file\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
